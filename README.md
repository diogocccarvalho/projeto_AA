-----

# Projeto de Sistemas Multi-Agente (SMA) - Simula√ß√£o e Aprendizagem

Este projeto implementa um ambiente de simula√ß√£o para agentes aut√≥nomos, explorando duas abordagens distintas de Intelig√™ncia Artificial: **Aprendizagem por Refor√ßo (Q-Learning)** e **Computa√ß√£o Evolucion√°ria (Neuroevolu√ß√£o)**.

O objetivo foi criar agentes capazes de resolver problemas de navega√ß√£o e recolha de recursos em ambientes complexos, din√¢micos e hostis.

## üë• Equipa

| N√∫mero de Aluno | Nome |
| :--- | :--- |
| **00000** | **Nome do Aluno 1** |
| **00000** | **Nome do Aluno 2** |
| **00000** | **Nome do Aluno 3** |

-----

## ü§ñ Declara√ß√£o de Uso de IA (Honestidade Acad√©mica)

Este projeto foi desenvolvido com a assist√™ncia de ferramentas de Intelig√™ncia Artificial (LLMs) como copiloto de desenvolvimento. A IA foi utilizada para:

  * **Refactoring e Otimiza√ß√£o:** Limpeza de c√≥digo, estrutura√ß√£o de classes e implementa√ß√£o de *multiprocessing* para treino paralelo.
  * **Ajuste Fino (Fine-tuning):** Sugest√£o de hiperpar√¢metros cr√≠ticos (tamanho de camadas escondidas, taxas de penalidade, decay de epsilon) para acelerar o processo de tentativa-erro.
  * **Depura√ß√£o L√≥gica:** Identifica√ß√£o de falhas em comportamentos de loop ("back-and-forth") e conflitos matem√°ticos nas tabelas de penalidade.
  * **Documenta√ß√£o:** Gera√ß√£o de estruturas para relat√≥rios e este README.

A l√≥gica arquitetural, a defini√ß√£o dos problemas e a valida√ß√£o final dos comportamentos emergentes foram inteiramente realizadas pela equipa.

-----

## üöÄ Funcionalidades Principais

### 1\. Ambientes Estoc√°sticos e Robustez

Ao contr√°rio de ambientes est√°ticos, o nosso simulador utiliza **Domain Randomization**. Em cada epis√≥dio de treino, o tamanho do mapa (20x20 a 50x50), o n√∫mero de obst√°culos, a posi√ß√£o dos recursos e os spawns mudam. Isto garante que os agentes n√£o "decoram" o mapa, mas sim aprendem a **navegar** e generalizar para qualquer situa√ß√£o.

### 2\. Heterogeneidade de Agentes

O sistema suporta a execu√ß√£o simult√¢nea de diferentes tipos de "c√©rebros":

  * **Agentes Q-Learning (Tabela):** Aprendizagem baseada em estados discretos, com mem√≥ria de curto prazo e consci√™ncia da a√ß√£o anterior.
  * **Agentes Evolucion√°rios (Rede Neuronal):** Redes *Feed-forward* (Input 15 -\> Hidden 40 -\> Output 6) cujos pesos s√£o evolu√≠dos atrav√©s de um Algoritmo Gen√©tico (Sele√ß√£o por Torneio, Elitismo e Muta√ß√£o Gaussiana).

### 3\. Modos de Competi√ß√£o e Coopera√ß√£o

  * **Competi√ß√£o:** M√∫ltiplos agentes competem pelos mesmos recursos limitados.
  * **Sentido de Urg√™ncia:** No cen√°rio do Farol, o treino foca-se no score do **Vencedor** (Max Score), incentivando a velocidade em vez da seguran√ßa excessiva.

### 4\. Treino Massivo

Implement√°mos paralelismo no treino gen√©tico para permitir popula√ß√µes grandes (200+ indiv√≠duos) e treinos longos (20.000+ epis√≥dios no Q-Learning) para superar a complexidade combinat√≥ria da Recole√ß√£o.

-----

## üß† Algoritmos e Mecanismos Implementados

### Q-Learning (Tabular)

  * **Estado Complexo:** Otimizado para evitar a "cegueira de estado". O tuplo de estado inclui:
      * Dire√ß√£o do Alvo (Quadrante).
      * Dist√¢ncia Discreta (4 n√≠veis: Em cima, Muito Perto, Perto, Longe).
      * Leitura de Sensores (Obst√°culos em 8 dire√ß√µes).
      * Estado de Carga (apenas na Recole√ß√£o).
      * **√öltima A√ß√£o:** Adicionada para que o agente saiba de onde veio.
  * **Explora√ß√£o:** Implementa√ß√£o de `Epsilon-Greedy` com decaimento lento (`0.99975`) para garantir explora√ß√£o profunda (at√© ao epis√≥dio 18.000).

### Neuroevolu√ß√£o (Algoritmo Gen√©tico)

  * **C√©rebro:** Uma Rede Neuronal Densa (Fully Connected).
      * *Hidden Layer:* Aumentada de 10 para **50 neur√≥nios** para permitir ao agente aprender a l√≥gica condicional complexa de "Alternar entre Busca e Entrega".
  * **Evolu√ß√£o:** A cada gera√ß√£o, os melhores agentes s√£o preservados (Elitismo) e os restantes s√£o criados por cruzamento e muta√ß√£o dos vencedores.

-----

## ‚ö†Ô∏è Dificuldades Encontradas e Solu√ß√µes T√©cnicas

Durante o desenvolvimento, enfrent√°mos v√°rios desafios de comportamento emergente indesejado.

| Problema | Sintoma Detalhado | Solu√ß√£o T√©cnica Implementada |
| :--- | :--- | :--- |
| **Oscila√ß√£o "Back-and-Forth"** | Mesmo em espa√ßo aberto, o agente vibrava entre Norte e Sul indefinidamente. | **Estado Estendido:** O agente sofria de *State Aliasing* (o estado na c√©lula A era igual ao estado na c√©lula B). Adicion√°mos a `self.acao_anterior` ao estado do Q-Learning para quebrar a simetria. |
| **Cegueira de Proximidade** | O agente chegava a 1 bloco do recurso e parava, "achando" que j√° tinha chegado, mas falhava a a√ß√£o de Recolher. | **Refinamento de Sensores:** Alter√°mos a `distancia_discreta` para ter precis√£o cir√∫rgica: Distinguir explicitamente Dist√¢ncia 0 (Em cima) de Dist√¢ncia \< 2 (Adjacente). |
| **O "C√≠rculo da Morte"** | Em mapas grandes (80x80), o agente entrava em estruturas complexas, dava a volta e esquecia-se que j√° tinha estado ali, entrando em loop infinito. | **Aumento de Mem√≥ria:** O `TAMANHO_HISTORICO` de posi√ß√µes visitadas foi aumentado, permitindo ao agente reconhecer loops muito maiores. |
| **Matem√°tica da Pregui√ßa** | O agente preferia ficar parado a bater numa parede infinitamente do que tentar sair de um beco sem sa√≠da. | **Reequil√≠brio de Penalidades:** A penalidade por bater na parede (-2.0) foi tornada superior √† penalidade de repeti√ß√£o (-1.5), tornando matematicamente vantajoso voltar para tr√°s. |
| **Score Negativo (-3000)** | Na recole√ß√£o, agentes morriam por timeout sem apanhar nada, aprendendo apenas a "n√£o morrer". | **For√ßa Bruta:** Aument√°mos a camada escondida da rede neuronal (10-\>40) e o n√∫mero de epis√≥dios Q-Learning (1k -\> 20k) para for√ßar a descoberta da recompensa esparsa. |

-----

## üõ†Ô∏è Como Executar

O projeto possui um ficheiro `main.py` robusto com v√°rios modos de execu√ß√£o.

### Instala√ß√£o

```bash
pip install numpy matplotlib
```

### Modos de Execu√ß√£o

**1. A "Demo Gigante" (Apresenta√ß√£o Final)**
Corre a sequ√™ncia completa: Farol, Recolha e mostra os gr√°ficos de evolu√ß√£o.

```bash
python main.py --modo DEMO_GIGANTE
```

**2. Treino Completo (Recomendado para gerar novos c√©rebros)**
Aten√ß√£o: Este modo demora imenso tempo a executar.

```bash
python main.py --modo TREINO_Q_EVO_ALL
```

**3. Demonstra√ß√µes Espec√≠ficas**

```bash
# Ver agentes Q-Learning no Farol
python main.py --modo DEMO_Q --cenario FAROL

# Ver agentes Evolucion√°rios na Recolha
python main.py --modo DEMO_EVO --cenario RECOLECAO
```

**4. Torneio (Equipa Q vs Equipa Evo)**

```bash
python main.py --modo DEMO_TEAMS --cenario RECOLECAO
```

-----

*Projeto realizado no √¢mbito da unidade curricular de Agentes Aut√≥nomos, 2025.*